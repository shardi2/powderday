---------------------------------------
Begin Slurm Prolog: Aug-22-2024 16:50:42
Job ID:    1119261
User ID:   shardin31
Account:   gts-jw254-coda20
Job name:  Slurmshardinpowderdayrun
Partition: cpu-small
QOS:       inferno
---------------------------------------

CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



Lmod is automatically replacing "mvapich2/2.3.6-ouywal" with "openmpi/4.1.4".

yt : [INFO     ] 2024-08-22 16:51:02,356 Global parallel computation enabled: 5 / 32
yt : [INFO     ] 2024-08-22 16:51:02,362 Global parallel computation enabled: 19 / 32
yt : [INFO     ] 2024-08-22 16:51:02,356 Global parallel computation enabled: 1 / 32
yt : [INFO     ] 2024-08-22 16:51:02,357 Global parallel computation enabled: 14 / 32
yt : [INFO     ] 2024-08-22 16:51:02,357 Global parallel computation enabled: 12 / 32
yt : [INFO     ] 2024-08-22 16:51:02,358 Global parallel computation enabled: 2 / 32
yt : [INFO     ] 2024-08-22 16:51:02,359 Global parallel computation enabled: 0 / 32
yt : [INFO     ] 2024-08-22 16:51:02,360 Global parallel computation enabled: 10 / 32
yt : [INFO     ] 2024-08-22 16:51:02,360 Global parallel computation enabled: 7 / 32
yt : [INFO     ] 2024-08-22 16:51:02,360 Global parallel computation enabled: 11 / 32
yt : [INFO     ] 2024-08-22 16:51:02,361 Global parallel computation enabled: 6 / 32
yt : [INFO     ] 2024-08-22 16:51:02,361 Global parallel computation enabled: 13 / 32
yt : [INFO     ] 2024-08-22 16:51:02,361 Global parallel computation enabled: 15 / 32
yt : [INFO     ] 2024-08-22 16:51:02,361 Global parallel computation enabled: 4 / 32
yt : [INFO     ] 2024-08-22 16:51:02,361 Global parallel computation enabled: 9 / 32
yt : [INFO     ] 2024-08-22 16:51:02,361 Global parallel computation enabled: 8 / 32
yt : [INFO     ] 2024-08-22 16:51:02,384 Global parallel computation enabled: 3 / 32
yt : [INFO     ] 2024-08-22 16:51:02,362 Global parallel computation enabled: 27 / 32
yt : [INFO     ] 2024-08-22 16:51:02,362 Global parallel computation enabled: 17 / 32
yt : [INFO     ] 2024-08-22 16:51:02,362 Global parallel computation enabled: 20 / 32
yt : [INFO     ] 2024-08-22 16:51:02,362 Global parallel computation enabled: 25 / 32
yt : [INFO     ] 2024-08-22 16:51:02,362 Global parallel computation enabled: 26 / 32
yt : [INFO     ] 2024-08-22 16:51:02,362 Global parallel computation enabled: 30 / 32
yt : [INFO     ] 2024-08-22 16:51:02,363 Global parallel computation enabled: 28 / 32
yt : [INFO     ] 2024-08-22 16:51:02,363 Global parallel computation enabled: 18 / 32
yt : [INFO     ] 2024-08-22 16:51:02,363 Global parallel computation enabled: 22 / 32
yt : [INFO     ] 2024-08-22 16:51:02,364 Global parallel computation enabled: 29 / 32
yt : [INFO     ] 2024-08-22 16:51:02,364 Global parallel computation enabled: 16 / 32
yt : [INFO     ] 2024-08-22 16:51:02,369 Global parallel computation enabled: 24 / 32
yt : [INFO     ] 2024-08-22 16:51:02,374 Global parallel computation enabled: 21 / 32
yt : [INFO     ] 2024-08-22 16:51:02,374 Global parallel computation enabled: 23 / 32
yt : [INFO     ] 2024-08-22 16:51:02,383 Global parallel computation enabled: 31 / 32
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Loading tree roots:   0%|          | 0/460025722 [00:00<?, ?it/s]Loading tree roots:   6%|▋         | 29109801/460025722 [00:00<00:01, 291063590.10it/s]Loading tree roots:  13%|█▎        | 58666537/460025722 [00:00<00:01, 293706723.88it/s]Loading tree roots:  19%|█▉        | 88038953/460025722 [00:00<00:01, 269651057.98it/s]Loading tree roots:  25%|██▌       | 116874823/460025722 [00:00<00:01, 276656876.78it/s]Loading tree roots:  32%|███▏      | 145428057/460025722 [00:00<00:01, 279749007.93it/s]Loading tree roots:  38%|███▊      | 173907577/460025722 [00:00<00:01, 281418071.94it/s]Loading tree roots:  44%|████▍     | 202133148/460025722 [00:00<00:00, 281351793.36it/s]Loading tree roots:  50%|█████     | 230325942/460025722 [00:00<00:00, 259113572.54it/s]Loading tree roots:  56%|█████▌    | 258141974/460025722 [00:00<00:00, 264680578.92it/s]Loading tree roots:  62%|██████▏   | 285278008/460025722 [00:01<00:00, 266649560.Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P016 yt : [ERROR    ] 2024-08-22 16:51:04,250 TypeError: 'generator' object is not subscriptable
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
01it/s]Loading tree roots:  68%|██████▊   | 312143725/460025722 [00:01<00:00, 263587388.08it/s]Loading tree roots:  74%|███████▎  | 338640844/460025722 [00:01<00:00, 263711730.79it/s]Loading tree roots:  79%|███████▉  | 365109244/460025722 [00:01<00:00, 262986509.57it/s]Loading tree roots:  85%|████████▌ | 391475317/460025722 [00:01<00:00, 262048401.36it/s]Loading tree roots:  91%|█████████ | 417726722/460025722 [00:01<00:00, 261794905.50it/s]Loading tree roots:  97%|█████████▋| 443941244/460025722 [00:01<00:00, 260889204.66it/s]Loading tree roots: 100%|██████████| 460025722/460025722 [00:01<00:00, 268107922.20it/s]slurmstepd: error: *** STEP 1119261.0 ON atl1-1-02-014-15-2 CANCELLED AT 2024-08-22T16:51:04 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: atl1-1-02-014-19-1: task 16: Exited with exit code 1
srun: error: atl1-1-02-014-19-1: tasks 17-31: Killed
srun: error: atl1-1-02-014-15-2: tasks 0-15: Killed
---------------------------------------
Begin Slurm Epilog: Aug-22-2024 16:51:04
Job ID:        1119261
Array Job ID:  _4294967294
User ID:       shardin31
Account:       gts-jw254-coda20
Job name:      Slurmshardinpowderdayrun
Resources:     cpu=32,mem=200G,node=2
Rsrc Used:     cput=00:11:44,vmem=8K,walltime=00:00:22,mem=0,energy_used=0
Partition:     cpu-small
QOS:           inferno
Nodes:         atl1-1-02-014-15-2,atl1-1-02-014-19-1
---------------------------------------
