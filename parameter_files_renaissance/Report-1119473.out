---------------------------------------
Begin Slurm Prolog: Aug-22-2024 17:11:49
Job ID:    1119473
User ID:   shardin31
Account:   gts-jw254-coda20
Job name:  Slurmshardinpowderdayrun
Partition: cpu-small
QOS:       inferno
---------------------------------------

CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



Lmod is automatically replacing "mvapich2/2.3.6-ouywal" with "openmpi/4.1.4".

yt : [INFO     ] 2024-08-22 17:12:08,171 Global parallel computation enabled: 3 / 32
yt : [INFO     ] 2024-08-22 17:12:08,172 Global parallel computation enabled: 17 / 32
yt : [INFO     ] 2024-08-22 17:12:08,171 Global parallel computation enabled: 5 / 32
yt : [INFO     ] 2024-08-22 17:12:08,172 Global parallel computation enabled: 11 / 32
yt : [INFO     ] 2024-08-22 17:12:08,172 Global parallel computation enabled: 13 / 32
yt : [INFO     ] 2024-08-22 17:12:08,173 Global parallel computation enabled: 4 / 32
yt : [INFO     ] 2024-08-22 17:12:08,173 Global parallel computation enabled: 14 / 32
yt : [INFO     ] 2024-08-22 17:12:08,174 Global parallel computation enabled: 1 / 32
yt : [INFO     ] 2024-08-22 17:12:08,174 Global parallel computation enabled: 9 / 32
yt : [INFO     ] 2024-08-22 17:12:08,174 Global parallel computation enabled: 2 / 32
yt : [INFO     ] 2024-08-22 17:12:08,175 Global parallel computation enabled: 0 / 32
yt : [INFO     ] 2024-08-22 17:12:08,175 Global parallel computation enabled: 8 / 32
yt : [INFO     ] 2024-08-22 17:12:08,175 Global parallel computation enabled: 6 / 32
yt : [INFO     ] 2024-08-22 17:12:08,172 Global parallel computation enabled: 21 / 32
yt : [INFO     ] 2024-08-22 17:12:08,176 Global parallel computation enabled: 10 / 32
yt : [INFO     ] 2024-08-22 17:12:08,172 Global parallel computation enabled: 25 / 32
yt : [INFO     ] 2024-08-22 17:12:08,176 Global parallel computation enabled: 15 / 32
yt : [INFO     ] 2024-08-22 17:12:08,172 Global parallel computation enabled: 19 / 32
yt : [INFO     ] 2024-08-22 17:12:08,176 Global parallel computation enabled: 12 / 32
yt : [INFO     ] 2024-08-22 17:12:08,173 Global parallel computation enabled: 31 / 32
yt : [INFO     ] 2024-08-22 17:12:08,185 Global parallel computation enabled: 7 / 32
yt : [INFO     ] 2024-08-22 17:12:08,174 Global parallel computation enabled: 28 / 32
yt : [INFO     ] 2024-08-22 17:12:08,175 Global parallel computation enabled: 29 / 32
yt : [INFO     ] 2024-08-22 17:12:08,175 Global parallel computation enabled: 23 / 32
yt : [INFO     ] 2024-08-22 17:12:08,176 Global parallel computation enabled: 20 / 32
yt : [INFO     ] 2024-08-22 17:12:08,176 Global parallel computation enabled: 18 / 32
yt : [INFO     ] 2024-08-22 17:12:08,176 Global parallel computation enabled: 22 / 32
yt : [INFO     ] 2024-08-22 17:12:08,177 Global parallel computation enabled: 16 / 32
yt : [INFO     ] 2024-08-22 17:12:08,177 Global parallel computation enabled: 27 / 32
yt : [INFO     ] 2024-08-22 17:12:08,177 Global parallel computation enabled: 26 / 32
yt : [INFO     ] 2024-08-22 17:12:08,177 Global parallel computation enabled: 24 / 32
yt : [INFO     ] 2024-08-22 17:12:08,178 Global parallel computation enabled: 30 / 32
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Loading tree roots:   0%|          | 0/460025722 [00:00<?, ?it/s]Loading tree roots:   6%|▋         | 28872233/460025722 [00:00<00:01, 288698514.44it/s]Loading tree roots:  13%|█▎        | 58764841/460025722 [00:00<00:01, 294710557.39it/s]Loading tree roots:  19%|█▉        | 88239657/460025722 [00:00<00:01, 277578914.08it/s]Loading tree roots:  26%|██▌       | 117542471/460025722 [00:00<00:01, 283448760.22it/s]Loading tree roots:  32%|███▏      | 146357854/460025722 [00:00<00:01, 285105152.11it/s]Loading tree roots:  38%|███▊      | 175341177/460025722 [00:00<00:00, 286682296.43it/s]Loading tree roots:  44%|████▍     | 204115612/460025722 [00:00<00:00, 287020624.36it/s]Loading tree roots:  51%|█████     | 232861366/460025722 [00:00<00:00, 287157285.32it/s]Loading tree roots:  57%|█████▋    | 261598998/460025722 [00:00<00:00, 287205055.50it/s]Loading tree roots:  63%|██████▎   | 290336570/460025722 [00:01<00:00, 283666075.Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P017 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P021 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P023 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P016 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P018 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P026 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P029 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P019 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P020 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P024 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P025 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P030 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
P017 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 17.
P023 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 23.
P021 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 21.
P016 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 16.
P018 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 18.
P025 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 25.
P026 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 26.
P029 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 29.
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P031 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
P019 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 19.
P020 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 20.
P024 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 24.
P030 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 30.
P031 yt : [ERROR    ] 2024-08-22 17:12:09,881 Error occurred on rank 31.
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P028 yt : [ERROR    ] 2024-08-22 17:12:09,881 TypeError: 'generator' object is not subscriptable
P028 yt : [ERROR    ] 2024-08-22 17:12:09,882 Error occurred on rank 28.
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 19 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 211, in <module>
    for node in trees["forest"][:5]:
P022 yt : [ERROR    ] 2024-08-22 17:12:09,883 TypeError: 'generator' object is not subscriptable
P022 yt : [ERROR    ] 2024-08-22 17:12:09,883 Error occurred on rank 22.
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
47it/s]Loading tree roots:  69%|██████▉   | 318721921/460025722 [00:01<00:00, 281466882.02it/s]Loading tree roots:  75%|███████▌  | 346886102/460025722 [00:01<00:00, 280482103.01it/s]Loading tree roots:  82%|████████▏ | 374943773/460025722 [00:01<00:00, 277938150.94it/s]Loading tree roots:  88%|████████▊ | 402747583/460025722 [00:01<00:00, 275230177.17it/s]Loading tree roots:  94%|█████████▎| 430281026/460025722 [00:01<00:00, 273521794.63it/s]slurmstepd: error: *** STEP 1119473.0 ON atl1-1-02-014-14-2 CANCELLED AT 2024-08-22T17:12:09 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: atl1-1-02-014-19-1: tasks 16-30: Killed
srun: error: atl1-1-02-014-19-1: task 31: Exited with exit code 1
srun: error: atl1-1-02-014-14-2: tasks 0-15: Killed
---------------------------------------
Begin Slurm Epilog: Aug-22-2024 17:12:10
Job ID:        1119473
Array Job ID:  _4294967294
User ID:       shardin31
Account:       gts-jw254-coda20
Job name:      Slurmshardinpowderdayrun
Resources:     cpu=32,mem=200G,node=2
Rsrc Used:     cput=00:11:44,vmem=24K,walltime=00:00:22,mem=0,energy_used=0
Partition:     cpu-small
QOS:           inferno
Nodes:         atl1-1-02-014-14-2,atl1-1-02-014-19-1
---------------------------------------
