---------------------------------------
Begin Slurm Prolog: Apr-02-2024 09:31:50
Job ID:    5481058
User ID:   shardin31
Account:   gts-jw254-coda20
Job name:  Slurmshardinpowderdayrun
Partition: cpu-small
QOS:       inferno
---------------------------------------

CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



Lmod is automatically replacing "mvapich2/2.3.6-ouywal" with "openmpi/4.1.4".

yt : [INFO     ] 2024-04-02 09:32:11,069 Global parallel computation enabled: 35 / 48
yt : [INFO     ] 2024-04-02 09:32:11,071 Global parallel computation enabled: 1 / 48
yt : [INFO     ] 2024-04-02 09:32:11,069 Global parallel computation enabled: 25 / 48
yt : [INFO     ] 2024-04-02 09:32:11,069 Global parallel computation enabled: 39 / 48
yt : [INFO     ] 2024-04-02 09:32:11,070 Global parallel computation enabled: 37 / 48
yt : [INFO     ] 2024-04-02 09:32:11,070 Global parallel computation enabled: 41 / 48
yt : [INFO     ] 2024-04-02 09:32:11,071 Global parallel computation enabled: 44 / 48
yt : [INFO     ] 2024-04-02 09:32:11,072 Global parallel computation enabled: 40 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 42 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 43 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 26 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 46 / 48
yt : [INFO     ] 2024-04-02 09:32:11,074 Global parallel computation enabled: 27 / 48
yt : [INFO     ] 2024-04-02 09:32:11,074 Global parallel computation enabled: 38 / 48
yt : [INFO     ] 2024-04-02 09:32:11,075 Global parallel computation enabled: 31 / 48
yt : [INFO     ] 2024-04-02 09:32:11,075 Global parallel computation enabled: 34 / 48
yt : [INFO     ] 2024-04-02 09:32:11,075 Global parallel computation enabled: 45 / 48
yt : [INFO     ] 2024-04-02 09:32:11,075 Global parallel computation enabled: 28 / 48
yt : [INFO     ] 2024-04-02 09:32:11,076 Global parallel computation enabled: 33 / 48
yt : [INFO     ] 2024-04-02 09:32:11,076 Global parallel computation enabled: 36 / 48
yt : [INFO     ] 2024-04-02 09:32:11,076 Global parallel computation enabled: 47 / 48
yt : [INFO     ] 2024-04-02 09:32:11,080 Global parallel computation enabled: 30 / 48
yt : [INFO     ] 2024-04-02 09:32:11,084 Global parallel computation enabled: 29 / 48
yt : [INFO     ] 2024-04-02 09:32:11,086 Global parallel computation enabled: 32 / 48
yt : [INFO     ] 2024-04-02 09:32:11,090 Global parallel computation enabled: 24 / 48
yt : [INFO     ] 2024-04-02 09:32:11,071 Global parallel computation enabled: 18 / 48
yt : [INFO     ] 2024-04-02 09:32:11,071 Global parallel computation enabled: 10 / 48
yt : [INFO     ] 2024-04-02 09:32:11,072 Global parallel computation enabled: 3 / 48
yt : [INFO     ] 2024-04-02 09:32:11,072 Global parallel computation enabled: 23 / 48
yt : [INFO     ] 2024-04-02 09:32:11,072 Global parallel computation enabled: 9 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 4 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 14 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 11 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 21 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 22 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 12 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 2 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 8 / 48
yt : [INFO     ] 2024-04-02 09:32:11,073 Global parallel computation enabled: 19 / 48
yt : [INFO     ] 2024-04-02 09:32:11,074 Global parallel computation enabled: 15 / 48
yt : [INFO     ] 2024-04-02 09:32:11,074 Global parallel computation enabled: 0 / 48
yt : [INFO     ] 2024-04-02 09:32:11,074 Global parallel computation enabled: 17 / 48
yt : [INFO     ] 2024-04-02 09:32:11,075 Global parallel computation enabled: 13 / 48
yt : [INFO     ] 2024-04-02 09:32:11,076 Global parallel computation enabled: 6 / 48
yt : [INFO     ] 2024-04-02 09:32:11,076 Global parallel computation enabled: 7 / 48
yt : [INFO     ] 2024-04-02 09:32:11,077 Global parallel computation enabled: 16 / 48
yt : [INFO     ] 2024-04-02 09:32:11,085 Global parallel computation enabled: 20 / 48
yt : [INFO     ] 2024-04-02 09:32:11,105 Global parallel computation enabled: 5 / 48
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P012 yt : [ERROR    ] 2024-04-02 09:32:11,366 NameError: name 'max_stellar_mass_size' is not defined
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P016 yt : [ERROR    ] 2024-04-02 09:32:11,366 NameError: name 'max_stellar_mass_size' is not defined
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P014 yt : [ERROR    ] 2024-04-02 09:32:11,366 NameError: name 'max_stellar_mass_size' is not defined
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P004 yt : [ERROR    ] 2024-04-02 09:32:11,366 NameError: name 'max_stellar_mass_size' is not defined
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P008 yt : [ERROR    ] 2024-04-02 09:32:11,366 NameError: name 'max_stellar_mass_size' is not defined
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P006 yt : [ERROR    ] 2024-04-02 09:32:11,367 NameError: name 'max_stellar_mass_size' is not defined
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P022 yt : [ERROR    ] 2024-04-02 09:32:11,367 NameError: name 'max_stellar_mass_size' is not defined
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 12 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P023 yt : [ERROR    ] 2024-04-02 09:32:11,367 NameError: name 'max_stellar_mass_size' is not defined
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P018 yt : [ERROR    ] 2024-04-02 09:32:11,367 NameError: name 'max_stellar_mass_size' is not defined
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P005 yt : [ERROR    ] 2024-04-02 09:32:11,367 NameError: name 'max_stellar_mass_size' is not defined
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P003 yt : [ERROR    ] 2024-04-02 09:32:11,367 NameError: name 'max_stellar_mass_size' is not defined
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P010 yt : [ERROR    ] 2024-04-02 09:32:11,368 NameError: name 'max_stellar_mass_size' is not defined
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P009 yt : [ERROR    ] 2024-04-02 09:32:11,368 NameError: name 'max_stellar_mass_size' is not defined
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P021 yt : [ERROR    ] 2024-04-02 09:32:11,368 NameError: name 'max_stellar_mass_size' is not defined
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P011 yt : [ERROR    ] 2024-04-02 09:32:11,368 NameError: name 'max_stellar_mass_size' is not defined
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P002 yt : [ERROR    ] 2024-04-02 09:32:11,369 NameError: name 'max_stellar_mass_size' is not defined
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P015 yt : [ERROR    ] 2024-04-02 09:32:11,369 NameError: name 'max_stellar_mass_size' is not defined
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P001 yt : [ERROR    ] 2024-04-02 09:32:11,369 NameError: name 'max_stellar_mass_size' is not defined
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 15 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 33, in <module>
    max_array_size = max_stellar_mass_size(node, maximum_array_size = max_array_size)
P013 yt : [ERROR    ] 2024-04-02 09:32:11,369 NameError: name 'max_stellar_mass_size' is not defined
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Loading tree roots:   0%|          | 0/7934216 [00:00<?, ?it/s]Loading tree roots: 100%|██████████| 7934216/7934216 [00:00<00:00, 269339521.40it/s]slurmstepd: error: *** STEP 5481058.0 ON atl1-1-02-015-32-1 CANCELLED AT 2024-04-02T09:32:11 ***
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: atl1-1-02-015-32-2: tasks 24-47: Killed
srun: error: atl1-1-02-015-32-1: tasks 0-7,9-15,17-23: Killed
srun: error: atl1-1-02-015-32-1: tasks 8,16: Exited with exit code 1
---------------------------------------
Begin Slurm Epilog: Apr-02-2024 09:32:11
Job ID:        5481058
Array Job ID:  _4294967294
User ID:       shardin31
Account:       gts-jw254-coda20
Job name:      Slurmshardinpowderdayrun
Resources:     cpu=48,mem=100G,node=2
Rsrc Used:     cput=00:16:48,vmem=28K,walltime=00:00:21,mem=0,energy_used=0
Partition:     cpu-small
QOS:           inferno
Nodes:         atl1-1-02-015-32-[1-2]
---------------------------------------
