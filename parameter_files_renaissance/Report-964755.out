---------------------------------------
Begin Slurm Prolog: Aug-05-2024 19:40:49
Job ID:    964755
User ID:   shardin31
Account:   gts-jw254-coda20
Job name:  Slurmshardinpowderdayrun
Partition: cpu-large
QOS:       inferno
---------------------------------------

CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



Lmod is automatically replacing "mvapich2/2.3.6-ouywal" with "openmpi/4.1.4".

yt : [INFO     ] 2024-08-05 19:41:22,424 Global parallel computation enabled: 7 / 32
yt : [INFO     ] 2024-08-05 19:41:22,431 Global parallel computation enabled: 21 / 32
yt : [INFO     ] 2024-08-05 19:41:22,431 Global parallel computation enabled: 31 / 32
yt : [INFO     ] 2024-08-05 19:41:22,431 Global parallel computation enabled: 17 / 32
yt : [INFO     ] 2024-08-05 19:41:22,431 Global parallel computation enabled: 26 / 32
yt : [INFO     ] 2024-08-05 19:41:22,432 Global parallel computation enabled: 19 / 32
yt : [INFO     ] 2024-08-05 19:41:22,432 Global parallel computation enabled: 30 / 32
yt : [INFO     ] 2024-08-05 19:41:22,432 Global parallel computation enabled: 24 / 32
yt : [INFO     ] 2024-08-05 19:41:22,433 Global parallel computation enabled: 20 / 32
yt : [INFO     ] 2024-08-05 19:41:22,433 Global parallel computation enabled: 18 / 32
yt : [INFO     ] 2024-08-05 19:41:22,433 Global parallel computation enabled: 27 / 32
yt : [INFO     ] 2024-08-05 19:41:22,434 Global parallel computation enabled: 28 / 32
yt : [INFO     ] 2024-08-05 19:41:22,434 Global parallel computation enabled: 22 / 32
yt : [INFO     ] 2024-08-05 19:41:22,435 Global parallel computation enabled: 16 / 32
yt : [INFO     ] 2024-08-05 19:41:22,435 Global parallel computation enabled: 29 / 32
yt : [INFO     ] 2024-08-05 19:41:22,435 Global parallel computation enabled: 25 / 32
yt : [INFO     ] 2024-08-05 19:41:22,444 Global parallel computation enabled: 23 / 32
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P021 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P028 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P019 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P022 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P024 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P025 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P027 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P029 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P030 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P016 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P018 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P020 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P023 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P026 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P031 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
P021 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 21.
P019 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 19.
yt : [INFO     ] 2024-08-05 19:41:22,426 Global parallel computation enabled: 13 / 32
yt : [INFO     ] 2024-08-05 19:41:22,426 Global parallel computation enabled: 12 / 32
yt : [INFO     ] 2024-08-05 19:41:22,426 Global parallel computation enabled: 14 / 32
yt : [INFO     ] 2024-08-05 19:41:22,427 Global parallel computation enabled: 6 / 32
yt : [INFO     ] 2024-08-05 19:41:22,427 Global parallel computation enabled: 3 / 32
yt : [INFO     ] 2024-08-05 19:41:22,428 Global parallel computation enabled: 5 / 32
yt : [INFO     ] 2024-08-05 19:41:22,428 Global parallel computation enabled: 10 / 32
yt : [INFO     ] 2024-08-05 19:41:22,429 Global parallel computation enabled: 2 / 32
yt : [INFO     ] 2024-08-05 19:41:22,430 Global parallel computation enabled: 0 / 32
yt : [INFO     ] 2024-08-05 19:41:22,430 Global parallel computation enabled: 1 / 32
yt : [INFO     ] 2024-08-05 19:41:22,430 Global parallel computation enabled: 4 / 32
yt : [INFO     ] 2024-08-05 19:41:22,430 Global parallel computation enabled: 9 / 32
yt : [INFO     ] 2024-08-05 19:41:22,431 Global parallel computation enabled: 11 / 32
yt : [INFO     ] 2024-08-05 19:41:22,431 Global parallel computation enabled: 15 / 32
yt : [INFO     ] 2024-08-05 19:41:22,431 Global parallel computation enabled: 8 / 32
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P010 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P013 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P003 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P005 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P006 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P008 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P009 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P011 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P014 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P000 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P001 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P002 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P004 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P007 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P015 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
P013 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 13.
P005 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 5.
P010 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 10.
P006 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 6.
P008 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 8.
P009 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 9.
P011 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 11.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
P014 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 14.
P000 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 0.
P002 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 2.
P003 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 3.
P007 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 7.
P015 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 15.
P004 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 4.
P001 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 1.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P012 yt : [ERROR    ] 2024-08-05 19:41:22,461 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
P012 yt : [ERROR    ] 2024-08-05 19:41:22,461 Error occurred on rank 12.
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 964755.0 ON atl1-1-02-004-2-1 CANCELLED AT 2024-08-05T19:41:22 ***
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
P028 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 28.
P030 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 30.
P022 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 22.
P024 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 24.
P025 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 25.
P027 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 27.
P016 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 16.
P023 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 23.
P029 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 29.
P018 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 18.
P020 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 20.
P031 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 31.
P026 yt : [ERROR    ] 2024-08-05 19:41:22,462 Error occurred on rank 26.
  File "analysis_pipeline.py", line 36, in <module>
    a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees")
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/data_structures/load.py", line 89, in load
    raise IOError(f"Could not determine arbor type for {filename}.")
P017 yt : [ERROR    ] 2024-08-05 19:41:22,462 OSError: Could not determine arbor type for /storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees.
P017 yt : [ERROR    ] 2024-08-05 19:41:22,463 Error occurred on rank 17.
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 19 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 27 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
srun: error: atl1-1-02-004-2-1: tasks 0-15: Killed
srun: error: atl1-1-02-004-4-2: tasks 16-18,20-31: Killed
srun: error: atl1-1-02-004-4-2: task 19: Exited with exit code 1
---------------------------------------
Begin Slurm Epilog: Aug-05-2024 19:41:22
Job ID:        964755
Array Job ID:  _4294967294
User ID:       shardin31
Account:       gts-jw254-coda20
Job name:      Slurmshardinpowderdayrun
Resources:     cpu=32,mem=720G,node=2
Rsrc Used:     cput=00:18:08,vmem=7080K,walltime=00:00:34,mem=7072K,energy_used=0
Partition:     cpu-large
QOS:           inferno
Nodes:         atl1-1-02-004-2-1,atl1-1-02-004-4-2
---------------------------------------
