---------------------------------------
Begin Slurm Prolog: Aug-22-2024 16:39:59
Job ID:    1119225
User ID:   shardin31
Account:   gts-jw254-coda20
Job name:  Slurmshardinpowderdayrun
Partition: cpu-small
QOS:       inferno
---------------------------------------

CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



Lmod is automatically replacing "mvapich2/2.3.6-ouywal" with "openmpi/4.1.4".

yt : [INFO     ] 2024-08-22 16:40:21,720 Global parallel computation enabled: 21 / 32
yt : [INFO     ] 2024-08-22 16:40:21,726 Global parallel computation enabled: 1 / 32
yt : [INFO     ] 2024-08-22 16:40:21,720 Global parallel computation enabled: 19 / 32
yt : [INFO     ] 2024-08-22 16:40:21,720 Global parallel computation enabled: 17 / 32
yt : [INFO     ] 2024-08-22 16:40:21,722 Global parallel computation enabled: 23 / 32
yt : [INFO     ] 2024-08-22 16:40:21,724 Global parallel computation enabled: 20 / 32
yt : [INFO     ] 2024-08-22 16:40:21,725 Global parallel computation enabled: 18 / 32
yt : [INFO     ] 2024-08-22 16:40:21,726 Global parallel computation enabled: 26 / 32
yt : [INFO     ] 2024-08-22 16:40:21,726 Global parallel computation enabled: 22 / 32
yt : [INFO     ] 2024-08-22 16:40:21,727 Global parallel computation enabled: 24 / 32
yt : [INFO     ] 2024-08-22 16:40:21,727 Global parallel computation enabled: 25 / 32
yt : [INFO     ] 2024-08-22 16:40:21,728 Global parallel computation enabled: 28 / 32
yt : [INFO     ] 2024-08-22 16:40:21,728 Global parallel computation enabled: 27 / 32
yt : [INFO     ] 2024-08-22 16:40:21,728 Global parallel computation enabled: 30 / 32
yt : [INFO     ] 2024-08-22 16:40:21,728 Global parallel computation enabled: 31 / 32
yt : [INFO     ] 2024-08-22 16:40:21,729 Global parallel computation enabled: 29 / 32
yt : [INFO     ] 2024-08-22 16:40:21,729 Global parallel computation enabled: 16 / 32
yt : [INFO     ] 2024-08-22 16:40:21,726 Global parallel computation enabled: 7 / 32
yt : [INFO     ] 2024-08-22 16:40:21,727 Global parallel computation enabled: 5 / 32
yt : [INFO     ] 2024-08-22 16:40:21,727 Global parallel computation enabled: 13 / 32
yt : [INFO     ] 2024-08-22 16:40:21,726 Global parallel computation enabled: 3 / 32
yt : [INFO     ] 2024-08-22 16:40:21,728 Global parallel computation enabled: 15 / 32
yt : [INFO     ] 2024-08-22 16:40:21,729 Global parallel computation enabled: 9 / 32
yt : [INFO     ] 2024-08-22 16:40:21,729 Global parallel computation enabled: 8 / 32
yt : [INFO     ] 2024-08-22 16:40:21,730 Global parallel computation enabled: 6 / 32
yt : [INFO     ] 2024-08-22 16:40:21,731 Global parallel computation enabled: 0 / 32
yt : [INFO     ] 2024-08-22 16:40:21,731 Global parallel computation enabled: 4 / 32
yt : [INFO     ] 2024-08-22 16:40:21,732 Global parallel computation enabled: 14 / 32
yt : [INFO     ] 2024-08-22 16:40:21,732 Global parallel computation enabled: 10 / 32
yt : [INFO     ] 2024-08-22 16:40:21,732 Global parallel computation enabled: 2 / 32
yt : [INFO     ] 2024-08-22 16:40:21,733 Global parallel computation enabled: 12 / 32
yt : [INFO     ] 2024-08-22 16:40:21,733 Global parallel computation enabled: 11 / 32
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Loading tree roots:   0%|          | 0/460025722 [00:00<?, ?it/s]Loading tree roots:   6%|▋         | 29249065/460025722 [00:00<00:01, 292460945.82it/s]Loading tree roots:  13%|█▎        | 59239977/460025722 [00:00<00:01, 296821811.10it/s]Loading tree roots:  19%|█▉        | 88923689/460025722 [00:00<00:01, 273271425.04it/s]Loading tree roots:  25%|██▌       | 116457031/460025722 [00:00<00:01, 269936088.90it/s]Loading tree roots:  31%|███       | 143568473/460025722 [00:00<00:01, 267080358.12it/s]Loading tree roots:  38%|███▊      | 172539513/460025722 [00:00<00:01, 274495446.11it/s]Loading tree roots:  44%|████▎     | 200740508/460025722 [00:00<00:00, 276890054.69it/s]Loading tree roots:  50%|████▉     | 229265067/460025722 [00:00<00:00, 279498586.50it/s]Loading tree roots:  56%|█████▌    | 257781526/460025722 [00:00<00:00, 281239003.56it/s]Loading tree roots:  62%|██████▏   | 286134072/460025722 [00:01<00:00, 281935959.57Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P002 yt : [ERROR    ] 2024-08-22 16:40:23,451 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P011 yt : [ERROR    ] 2024-08-22 16:40:23,451 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P014 yt : [ERROR    ] 2024-08-22 16:40:23,451 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P004 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P006 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P008 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P009 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P010 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P012 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P013 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P015 yt : [ERROR    ] 2024-08-22 16:40:23,452 AttributeError: 'str' object has no attribute 'arbor'
P002 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 2.
P006 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 6.
P008 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 8.
P011 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 11.
P009 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 9.
P010 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 10.
P012 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 12.
P015 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 15.
P004 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 4.
P013 yt : [ERROR    ] 2024-08-22 16:40:23,452 Error occurred on rank 13.
P014 yt : [ERROR    ] 2024-08-22 16:40:23,453 Error occurred on rank 14.
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 12 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 15 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
it/s]Loading tree roots:  68%|██████▊   | 314351477/460025722 [00:01<00:00, 279657963.62it/s]Loading tree roots:  74%|███████▍  | 342335443/460025722 [00:01<00:00, 277292953.71it/s]Loading tree roots:  80%|████████  | 370081809/460025722 [00:01<00:00, 276071141.16it/s]Loading tree roots:  86%|████████▋ | 397701261/460025722 [00:01<00:00, 274563857.02it/s]Loading tree roots:  92%|█████████▏| 425165098/460025722 [00:01<00:00, 272399540.80it/s]slurmstepd: error: *** STEP 1119225.0 ON atl1-1-02-015-7-2 CANCELLED AT 2024-08-22T16:40:23 ***
  File "analysis_pipeline.py", line 209, in <module>
    for tree in ytree.parallel_trees(trees, dynamic=dynamic):
  File "/storage/home/hcoda1/7/shardin31/.conda/envs/pd_environment_3/lib/python3.8/site-packages/ytree/utilities/parallel.py", line 114, in parallel_trees
    arbor = trees[0].arbor
P003 yt : [ERROR    ] 2024-08-22 16:40:23,457 AttributeError: 'str' object has no attribute 'arbor'
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: atl1-1-02-015-7-2: tasks 0-15: Killed
srun: error: atl1-1-02-015-12-1: tasks 16-31: Killed
---------------------------------------
Begin Slurm Epilog: Aug-22-2024 16:40:24
Job ID:        1119225
Array Job ID:  _4294967294
User ID:       shardin31
Account:       gts-jw254-coda20
Job name:      Slurmshardinpowderdayrun
Resources:     cpu=32,mem=200G,node=2
Rsrc Used:     cput=00:13:20,vmem=876K,walltime=00:00:25,mem=0,energy_used=0
Partition:     cpu-small
QOS:           inferno
Nodes:         atl1-1-02-015-7-2,atl1-1-02-015-12-1
---------------------------------------
