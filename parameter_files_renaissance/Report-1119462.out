---------------------------------------
Begin Slurm Prolog: Aug-22-2024 17:08:04
Job ID:    1119462
User ID:   shardin31
Account:   gts-jw254-coda20
Job name:  Slurmshardinpowderdayrun
Partition: cpu-small
QOS:       inferno
---------------------------------------

CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



Lmod is automatically replacing "mvapich2/2.3.6-ouywal" with "openmpi/4.1.4".

yt : [INFO     ] 2024-08-22 17:08:36,014 Global parallel computation enabled: 9 / 32
yt : [INFO     ] 2024-08-22 17:08:36,017 Global parallel computation enabled: 23 / 32
yt : [INFO     ] 2024-08-22 17:08:36,014 Global parallel computation enabled: 13 / 32
yt : [INFO     ] 2024-08-22 17:08:36,015 Global parallel computation enabled: 8 / 32
yt : [INFO     ] 2024-08-22 17:08:36,015 Global parallel computation enabled: 15 / 32
yt : [INFO     ] 2024-08-22 17:08:36,015 Global parallel computation enabled: 0 / 32
yt : [INFO     ] 2024-08-22 17:08:36,016 Global parallel computation enabled: 2 / 32
yt : [INFO     ] 2024-08-22 17:08:36,016 Global parallel computation enabled: 3 / 32
yt : [INFO     ] 2024-08-22 17:08:36,017 Global parallel computation enabled: 11 / 32
yt : [INFO     ] 2024-08-22 17:08:36,017 Global parallel computation enabled: 7 / 32
yt : [INFO     ] 2024-08-22 17:08:36,017 Global parallel computation enabled: 12 / 32
yt : [INFO     ] 2024-08-22 17:08:36,018 Global parallel computation enabled: 4 / 32
yt : [INFO     ] 2024-08-22 17:08:36,018 Global parallel computation enabled: 10 / 32
yt : [INFO     ] 2024-08-22 17:08:36,018 Global parallel computation enabled: 6 / 32
yt : [INFO     ] 2024-08-22 17:08:36,018 Global parallel computation enabled: 14 / 32
yt : [INFO     ] 2024-08-22 17:08:36,020 Global parallel computation enabled: 1 / 32
yt : [INFO     ] 2024-08-22 17:08:36,026 Global parallel computation enabled: 5 / 32
yt : [INFO     ] 2024-08-22 17:08:36,017 Global parallel computation enabled: 17 / 32
yt : [INFO     ] 2024-08-22 17:08:36,017 Global parallel computation enabled: 19 / 32
yt : [INFO     ] 2024-08-22 17:08:36,017 Global parallel computation enabled: 21 / 32
yt : [INFO     ] 2024-08-22 17:08:36,018 Global parallel computation enabled: 28 / 32
yt : [INFO     ] 2024-08-22 17:08:36,019 Global parallel computation enabled: 30 / 32
yt : [INFO     ] 2024-08-22 17:08:36,021 Global parallel computation enabled: 24 / 32
yt : [INFO     ] 2024-08-22 17:08:36,022 Global parallel computation enabled: 31 / 32
yt : [INFO     ] 2024-08-22 17:08:36,023 Global parallel computation enabled: 20 / 32
yt : [INFO     ] 2024-08-22 17:08:36,023 Global parallel computation enabled: 16 / 32
yt : [INFO     ] 2024-08-22 17:08:36,023 Global parallel computation enabled: 22 / 32
yt : [INFO     ] 2024-08-22 17:08:36,024 Global parallel computation enabled: 25 / 32
yt : [INFO     ] 2024-08-22 17:08:36,024 Global parallel computation enabled: 27 / 32
yt : [INFO     ] 2024-08-22 17:08:36,025 Global parallel computation enabled: 18 / 32
yt : [INFO     ] 2024-08-22 17:08:36,025 Global parallel computation enabled: 29 / 32
yt : [INFO     ] 2024-08-22 17:08:36,026 Global parallel computation enabled: 26 / 32
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Loading tree roots:   0%|          | 0/460025722 [00:00<?, ?it/s]Loading tree roots:   6%|▋         | 29445673/460025722 [00:00<00:01, 294449288.57it/s]Loading tree roots:  13%|█▎        | 59354665/460025722 [00:00<00:01, 297171291.49it/s]Loading tree roots:  19%|█▉        | 89075241/460025722 [00:00<00:01, 292067389.65it/s]Loading tree roots:  26%|██▌       | 118496839/460025722 [00:00<00:01, 292904914.14it/s]Loading tree roots:  32%|███▏      | 147914334/460025722 [00:00<00:01, 293348384.28it/s]Loading tree roots:  39%|███▊      | 177254009/460025722 [00:00<00:00, 291562553.46it/s]Loading tree roots:  45%|████▍     | 206417564/460025722 [00:00<00:00, 290840003.64it/s]Loading tree roots:  51%|█████     | 235507382/460025722 [00:00<00:00, 288934010.60it/s]Loading tree roots:  57%|█████▋    | 264404758/460025722 [00:00<00:00, 288008591.30it/s]Loading tree roots:  64%|██████▎   | 293207876/460025722 [00:01<00:00, 286135698.Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P026 yt : [ERROR    ] 2024-08-22 17:08:37,711 NameError: name 'tree' is not defined
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/RS-RP/rockstar_halos/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
P026 yt : [ERROR    ] 2024-08-22 17:08:37,711 Error occurred on rank 26.
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P019 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P023 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P025 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P017 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P021 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P028 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P029 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P031 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P022 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P020 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P027 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P018 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P016 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P024 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
P019 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 19.
P021 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 21.
P023 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 23.
P025 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 25.
P031 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 31.
P017 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 17.
P028 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 28.
P029 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 29.
  File "analysis_pipeline.py", line 211, in <module>
    for node in tree["forest"][:5]:
P030 yt : [ERROR    ] 2024-08-22 17:08:37,712 NameError: name 'tree' is not defined
P020 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 20.
P018 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 18.
P022 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 22.
P027 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 27.
P016 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 16.
P024 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 24.
P030 yt : [ERROR    ] 2024-08-22 17:08:37,712 Error occurred on rank 30.
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 19 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 27 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
58it/s]Loading tree roots:  70%|██████▉   | 321826701/460025722 [00:01<00:00, 283542225.61it/s]Loading tree roots:  76%|███████▌  | 350187492/460025722 [00:01<00:00, 281621650.78it/s]Loading tree roots:  82%|████████▏ | 378355757/460025722 [00:01<00:00, 279073422.90it/s]Loading tree roots:  88%|████████▊ | 406270144/460025722 [00:01<00:00, 277428753.81it/s]Loading tree roots:  94%|█████████▍| 434016578/460025722 [00:01<00:00, 275485272.20it/s]slurmstepd: error: *** STEP 1119462.0 ON atl1-1-02-014-14-2 CANCELLED AT 2024-08-22T17:08:37 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: atl1-1-02-014-19-1: tasks 16,18-31: Killed
srun: error: atl1-1-02-014-19-1: task 17: Exited with exit code 1
srun: error: atl1-1-02-014-14-2: tasks 0-15: Killed
---------------------------------------
Begin Slurm Epilog: Aug-22-2024 17:08:38
Job ID:        1119462
Array Job ID:  _4294967294
User ID:       shardin31
Account:       gts-jw254-coda20
Job name:      Slurmshardinpowderdayrun
Resources:     cpu=32,mem=200G,node=2
Rsrc Used:     cput=00:18:40,vmem=7064K,walltime=00:00:35,mem=7056K,energy_used=0
Partition:     cpu-small
QOS:           inferno
Nodes:         atl1-1-02-014-14-2,atl1-1-02-014-19-1
---------------------------------------
