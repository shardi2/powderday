---------------------------------------
Begin Slurm Prolog: Mar-27-2024 11:50:49
Job ID:    5338813
User ID:   shardin31
Account:   gts-jw254-coda20
Job name:  Slurmshardinpowderdayrun
Partition: cpu-small
QOS:       inferno
---------------------------------------

CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



Lmod is automatically replacing "mvapich2/2.3.6-ouywal" with "openmpi/4.1.4".

yt : [INFO     ] 2024-03-27 11:51:57,712 Global parallel computation enabled: 7 / 8
yt : [INFO     ] 2024-03-27 11:51:57,718 Global parallel computation enabled: 2 / 8
yt : [INFO     ] 2024-03-27 11:51:57,712 Global parallel computation enabled: 5 / 8
yt : [INFO     ] 2024-03-27 11:51:57,712 Global parallel computation enabled: 6 / 8
yt : [INFO     ] 2024-03-27 11:51:57,712 Global parallel computation enabled: 4 / 8
yt : [INFO     ] 2024-03-27 11:51:57,730 Global parallel computation enabled: 0 / 8
yt : [INFO     ] 2024-03-27 11:51:57,730 Global parallel computation enabled: 1 / 8
yt : [INFO     ] 2024-03-27 11:51:57,731 Global parallel computation enabled: 3 / 8
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 138, in <module>
    maximum_array_size = max_stellar_mass_size(node, maximum_array_size)
  File "analysis_pipeline.py", line 72, in max_stellar_mass_size
    sphere = get_yt_sphere(node)
  File "analysis_pipeline.py", line 120, in get_yt_sphere
    ds = node.ds
P002 yt : [ERROR    ] 2024-03-27 11:51:57,996 AttributeError: 'TreeNode' object has no attribute 'ds'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 138, in <module>
    maximum_array_size = max_stellar_mass_size(node, maximum_array_size)
  File "analysis_pipeline.py", line 72, in max_stellar_mass_size
    sphere = get_yt_sphere(node)
  File "analysis_pipeline.py", line 120, in get_yt_sphere
    ds = node.ds
P001 yt : [ERROR    ] 2024-03-27 11:51:58,004 AttributeError: 'TreeNode' object has no attribute 'ds'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 138, in <module>
    maximum_array_size = max_stellar_mass_size(node, maximum_array_size)
  File "analysis_pipeline.py", line 72, in max_stellar_mass_size
    sphere = get_yt_sphere(node)
  File "analysis_pipeline.py", line 120, in get_yt_sphere
    ds = node.ds
P004 yt : [ERROR    ] 2024-03-27 11:51:58,003 AttributeError: 'TreeNode' object has no attribute 'ds'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 138, in <module>
    maximum_array_size = max_stellar_mass_size(node, maximum_array_size)
  File "analysis_pipeline.py", line 72, in max_stellar_mass_size
    sphere = get_yt_sphere(node)
  File "analysis_pipeline.py", line 120, in get_yt_sphere
    ds = node.ds
P003 yt : [ERROR    ] 2024-03-27 11:51:58,010 AttributeError: 'TreeNode' object has no attribute 'ds'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 138, in <module>
    maximum_array_size = max_stellar_mass_size(node, maximum_array_size)
  File "analysis_pipeline.py", line 72, in max_stellar_mass_size
    sphere = get_yt_sphere(node)
  File "analysis_pipeline.py", line 120, in get_yt_sphere
    ds = node.ds
P007 yt : [ERROR    ] 2024-03-27 11:51:58,013 AttributeError: 'TreeNode' object has no attribute 'ds'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 7 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
Additional features and improved performance (usually) by saving this arbor with "save_arbor" and reloading:
	>>> a = ytree.load("/storage/home/hcoda1/0/jw254/data/SG64-2020/rockstar_halos-jhw/trees/tree_0_0_0.dat")
	>>> fn = a.save_arbor()
	>>> a = ytree.load(fn)
  File "analysis_pipeline.py", line 138, in <module>
    maximum_array_size = max_stellar_mass_size(node, maximum_array_size)
  File "analysis_pipeline.py", line 72, in max_stellar_mass_size
    sphere = get_yt_sphere(node)
  File "analysis_pipeline.py", line 120, in get_yt_sphere
    ds = node.ds
P006 yt : [ERROR    ] 2024-03-27 11:51:58,015 AttributeError: 'TreeNode' object has no attribute 'ds'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "analysis_pipeline.py", line 138, in <module>
    maximum_array_size = max_stellar_mass_size(node, maximum_array_size)
  File "analysis_pipeline.py", line 72, in max_stellar_mass_size
    sphere = get_yt_sphere(node)
  File "analysis_pipeline.py", line 120, in get_yt_sphere
    ds = node.ds
P005 yt : [ERROR    ] 2024-03-27 11:51:58,016 AttributeError: 'TreeNode' object has no attribute 'ds'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Loading tree roots:   0%|          | 0/7934216 [00:00<?, ?it/s]Loading tree roots: 100%|██████████| 7934216/7934216 [00:00<00:00, 336973722.43it/s]slurmstepd: error: *** STEP 5338813.0 ON atl1-1-02-009-30-1 CANCELLED AT 2024-03-27T11:51:58 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: atl1-1-02-009-30-2: tasks 4-7: Killed
srun: error: atl1-1-02-009-30-1: tasks 0-1,3: Killed
srun: error: atl1-1-02-009-30-1: task 2: Exited with exit code 1
---------------------------------------
Begin Slurm Epilog: Mar-27-2024 11:51:58
Job ID:        5338813
Array Job ID:  _4294967294
User ID:       shardin31
Account:       gts-jw254-coda20
Job name:      Slurmshardinpowderdayrun
Resources:     cpu=8,mem=100G,node=2
Rsrc Used:     cput=00:09:12,vmem=6804K,walltime=00:01:09,mem=5920K,energy_used=0
Partition:     cpu-small
QOS:           inferno
Nodes:         atl1-1-02-009-30-[1-2]
---------------------------------------
